{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPV1GBgKC60LB0o9GMI5p4i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrigakshipandey/seminar-mikroelektronik/blob/main/Task_4_LLaMa_2_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLOLVBrwWRTP",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log in to Hugging Face"
      ],
      "metadata": {
        "id": "4bsFLceKWpoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()  # Paste your Hugging Face token here (with model access)\n"
      ],
      "metadata": {
        "id": "AwpR3eGeWtT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "we_k0-_tXnrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-hf\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YymqS1TWXfLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture Summary\n",
        "\n",
        "\n",
        "| Parameter                              | Description                                                       | Why It Matters                                                           |\n",
        "| -------------------------------------- | ----------------------------------------------------------------- | ------------------------------------------------------------------------ |\n",
        "| `hidden_size`                          | Dimensionality of the embeddings and hidden layers (e.g. 4096)    | Controls how much information the model can store per token              |\n",
        "|\n",
        "| `num_hidden_layers`                    | Total number of Transformer blocks (e.g. 32 for LLaMA-2 7B)       | Deeper models usually have better generalization                         |\n",
        "| `num_attention_heads`                  | Number of attention heads per layer                               | Allows model to attend to different subspaces of input                   |\n",
        "| `num_key_value_heads`                  | Number of *shared* KV heads for multi-query attention (MQA)       | Improves inference speed & reduces memory (used in LLaMA 2/3)            |\n",
        "| `vocab_size`                           | Total number of unique tokens the model understands               | Larger vocab supports more languages, rare words                         |\n",
        "| `Max Context`              | Maximum context length (number of tokens) the model can attend to | Limits how long the input/output sequences can be                        |\n",
        "| `Rotary Dim` | Rotary embedding parameter (used for positional encoding)         | Affects how attention handles long-range dependencies                    |\n",
        "| `DropoutPro.`                  | Dropout probability in the hidden layers                          | Regularization to prevent overfitting (not always used during inference) |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RxqUWEUQcTZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = model.config\n",
        "\n",
        "print(\"Model Architecture Summary\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Model Type        : {config.model_type}\")\n",
        "print(f\"Hidden Size       : {config.hidden_size}\")\n",
        "print(f\"Num of Layers     : {config.num_hidden_layers}\")\n",
        "print(f\"Attention Heads   : {config.num_attention_heads}\")\n",
        "print(f\"KV Heads          : {getattr(config, 'num_key_value_heads', 'N/A')}\")\n",
        "print(f\"Vocab Size        : {config.vocab_size}\")\n",
        "print(f\"Max Context (Seq) : {getattr(config, 'max_position_embeddings', 'N/A')}\")\n",
        "print(f\"Rotary Dim        : {getattr(config, 'rope_theta', 'N/A')}\")\n",
        "print(f\"Dropout Prob.     : {config.hidden_dropout_prob if hasattr(config, 'hidden_dropout_prob') else 'N/A'}\")\n"
      ],
      "metadata": {
        "id": "2h8-O_4AXtj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The General Architecture of LLaMa\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TKl_90lEqC_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![llama2](https://miro.medium.com/1*CQs4ceLpN8tIN8QyezL2Ag.png)"
      ],
      "metadata": {
        "id": "bOi7-47NMLK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the [source code](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py), the model performs the following operations:\n",
        "\n",
        "- LlamaForCausalLM\n",
        "  - **Embedding**\n",
        "  - LlamaDecoderLayer * num_hidden_layers\n",
        "    - **LlamaRMSNorm** (input_layernorm)\n",
        "    - **LlamaAttention**\n",
        "      - Reshape for multi-head attention\n",
        "      - **Linear projections for Q, K, V**\n",
        "      - **RoPE**\n",
        "      - Update KV Cache\n",
        "      - **Attention**\n",
        "        - Compute Score for every query-key pair\n",
        "        - scaling\n",
        "        - attention_mask\n",
        "        - Softmax\n",
        "        - dropout\n",
        "        - multiply with value\n",
        "        - Reshape\n",
        "      - Linar Projection for O\n",
        "    - residual add\n",
        "    - LlamaRMSNorm (post_attention_layernorm)\n",
        "    - LlamaMLP\n",
        "    - residual add\n",
        "  - LlamaRMSNorm\n",
        "  - Linear"
      ],
      "metadata": {
        "id": "Yt8N7TE9Rz37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input"
      ],
      "metadata": {
        "id": "dXPcBWufiPzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Use the same repo as your model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "# Tokenize input text\n",
        "text = \"Hello LLM User\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "input_ids = inputs[\"input_ids\"]\n",
        "\n",
        "print(\"Token IDs:\", input_ids)\n",
        "print(\"Decoded back:\", tokenizer.decode(input_ids[0]))"
      ],
      "metadata": {
        "id": "DNowdke7UYgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input tensor of dimensions (1, 5).\n",
        "\n",
        "Where 1 is the Batch size and 5 is the number of token in the Input Sequence"
      ],
      "metadata": {
        "id": "U6UAJCbFB4Uz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding Layer\n",
        "Each token in the Input Sequence is a part of a pre defined vocabulary. (This model has a vocab_size of 32000.)\n",
        "\n",
        "We use the position of the token in the vocabulary to perform a lookup operation in the embedding matrix."
      ],
      "metadata": {
        "id": "9OpKDiHxobNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_layer = model.model.embed_tokens\n",
        "print(\"Embedding matrix shape:\", embed_layer.weight.shape)\n",
        "print(\"- Vocab size:\", embed_layer.weight.shape[0])\n",
        "print(\"- Hidden size:\", embed_layer.weight.shape[1])\n",
        "print(\"Note: Dtype:\", embed_layer.weight.dtype)"
      ],
      "metadata": {
        "id": "-ioXSjAqb70Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](https://www.3blue1brown.com/content/lessons/2024/gpt/token.png)"
      ],
      "metadata": {
        "id": "c-pJNw89wsZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "### Embedding: Description\n",
        "Embedding can be understood to carry the meaning of the token.\n",
        "\n",
        "Given our Input dimensaion (1, 5).\n",
        "\n",
        "And the embedding matrix shape (32000, 4096)\n",
        "\n",
        "After Embedding we'll get a tensor with dimensions (1, 5, 4096).\n",
        "\n",
        "---\n",
        "Additional Notes\n",
        "- In the Original Transformer Architecture, this was followed up by an **Absolute positional Encoding**.\n",
        "\n",
        "- In the LLaMa Model, we use **Rotary Positional Encoding** later with the Query and Key Matrix.\n"
      ],
      "metadata": {
        "id": "PHngHbFvEfvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "### Embedding Matrix: Static Memory Requirements\n",
        "Number of Parameters\n",
        "- 32000 * 4096 = 131,072,000\n",
        "\n",
        "Therefore, the Memory requirements for float16\n",
        "- 32000 * 4096 * (16/8) = 262144000 (over 262 MB)"
      ],
      "metadata": {
        "id": "q4t6F4XjmW8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding_static_memory():\n",
        "\n",
        "  module = model.model.embed_tokens\n",
        "  num_params = sum(p.numel() for p in module.parameters())\n",
        "  param_size = sum(p.numel() * p.element_size() for p in module.parameters())\n",
        "\n",
        "  print(\"\\nEmbedding parameters:\", num_params)\n",
        "  print(\"Embedding memory: {:.2f} MB\".format(num_params * 2 / (1024 * 1024)))  # fp16\n",
        "\n",
        "get_embedding_static_memory()"
      ],
      "metadata": {
        "id": "MIfjcvtri7Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "### Embedding: Operations per Token\n",
        "It's a lookup operation nearly no FLOPS are performed"
      ],
      "metadata": {
        "id": "YuOfhbbAqEeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "###  Embedding Forward Pass: Accessed Bytes\n",
        "To fetch the Embedding for 1 batch of seq length\n",
        "- 1 * seq * 4096 * (16/8)\n",
        "\n",
        "For the longest allowable sequence length (4096)\n",
        "- 1 * 4096 * 4096 * (16/8) = 33554432 (over 33 MB)\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "-N3qMYOPtOmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMS Norm\n",
        "Without normalisation, the magnitude of activations can explode or vanish as the depth increases. So any normalisation (LayerNorm, RMSNorm, etc.) helps deep transformers like LLaMA learn better and generalize faster.\n",
        "\n",
        "Layer Norm uses Recentering and Rescaling, on the contrary RMS norm uses the hypothesis that Rescaling and not Recentering is the major contributing factor for Normalization.\n",
        "\n",
        "- RMS Norm is popular because it requies fewer computations.\n",
        "\n",
        "Source code:\n",
        "```\n",
        "def forward(self, hidden_states):\n",
        "        input_dtype = hidden_states.dtype\n",
        "        hidden_states = hidden_states.to(torch.float32)\n",
        "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
        "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
        "        return self.weight * hidden_states.to(input_dtype)\n",
        "```"
      ],
      "metadata": {
        "id": "8ZkY9SWCMGa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: take the RMSNorm before the first self-attention\n",
        "rmsnorm = model.model.layers[0].input_layernorm\n",
        "\n",
        "# 1) The learnable scale parameter (w)\n",
        "print(\"Weight shape:\", rmsnorm.weight.shape)   # (hidden_size,)\n",
        "\n",
        "# 2) The epsilon value (fixed, not a Parameter)\n",
        "print(\"Epsilon:\", rmsnorm.variance_epsilon)"
      ],
      "metadata": {
        "id": "kGWCiJrXsDM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.fx import symbolic_trace\n",
        "from transformers.models.llama.modeling_llama import LlamaRMSNorm\n",
        "\n",
        "# Define the module\n",
        "hidden_dim = 4096\n",
        "rms = LlamaRMSNorm(hidden_dim)\n",
        "\n",
        "# Trace the computation graph\n",
        "rms_traced = symbolic_trace(rms)\n",
        "\n",
        "# Inspect individual nodes\n",
        "for node in rms_traced.graph.nodes:\n",
        "    print(node.op, node.target, node.args)\n"
      ],
      "metadata": {
        "id": "nuMNhXwSx5_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "### RMS Norm: Operations Breakdown\n",
        "\n",
        "- The input tensor x\n",
        "1. Cast input to float32 (even if model runs in fp16)\n",
        "2. Square every element\n",
        "3. Compute mean across the hidden dimension\n",
        "4. Add epsilon\n",
        "5. Reciprocal square root\n",
        "6. Multiply with x\n",
        "7. Retrieve learnable scaling parameter w\n",
        "8. Get dtype of original input\n",
        "9. Cast result back to original dtype\n",
        "10. Multiply elementwise by learnable weight w.\n",
        "- RMSNorm output"
      ],
      "metadata": {
        "id": "V9X29a2XymFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "### RMS Norm: Static Memory Requirements\n"
      ],
      "metadata": {
        "id": "eFI37jUZBnti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.llama.modeling_llama import LlamaRMSNorm\n",
        "def get_rmsnorm_static_memory():\n",
        "    hidden_dim = 4096\n",
        "    module = LlamaRMSNorm(hidden_dim)\n",
        "\n",
        "    num_params = sum(p.numel() for p in module.parameters())\n",
        "    param_size = sum(p.numel() * p.element_size() for p in module.parameters())\n",
        "\n",
        "    print(\"\\nRMSNorm parameters:\", num_params)\n",
        "    print(\"RMSNorm memory: {:.2f} MB\".format(num_params * 2 / (1024 * 1024)))  # 2 bytes per fp16 param\n",
        "\n",
        "get_rmsnorm_static_memory()"
      ],
      "metadata": {
        "id": "QWLl124NhQC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "### RMS Norm: Operations\n",
        "Let:\n",
        "\n",
        "batch = B, seq = S, hidden = D\n",
        "\n",
        "N = B·S·D\n",
        "\n",
        "1. `Cast:` x.to(float32)\n",
        "\n",
        "    FLOPs: 0\n",
        "\n",
        "2. `Square:` pow(x32, 2)\n",
        "\n",
        "    FLOPs: D (muliplications)\n",
        "\n",
        "3. `Mean:` mean(x32^2, dim=-1, keepdim=True) → shape (B,S,1)\n",
        "\n",
        "    FLOps: D-1 (additions) + 1 (divison: hardware dependent)\n",
        "    = D\n",
        "\n",
        "4. `Add epsilon:` add(eps) on (B,S,1)\n",
        "\n",
        "    FLOps: 1 (addition)\n",
        "\n",
        "5. `Reciprocal square root:`  rsqrt on (B,S,1)\n",
        "\n",
        "    FLOps: 1 (sqrt: hardware dependent) + 1 (reciprocal: hardware dependent)\n",
        "\n",
        "6. `Multiply with x:` mul(x32, inv_r) (broadcast (B,S,1) → (B,S,D))\n",
        "\n",
        "    FLOps: D\n",
        "\n",
        "7. `Retrieve w:` get_attr(weight) (γ, FP32)\n",
        "\n",
        "    FLOPs: 0\n",
        "\n",
        "8. `Get dtype of original input`\n",
        "\n",
        "    FLOPs: 0\n",
        "\n",
        "9. `Cast to original dtype:` y32.to(x.dtype) (back to input dtype)\n",
        "\n",
        "    FLOPs: 0\n",
        "\n",
        "10. `Multiply elementwise:` mul(y, weight) (broadcast γ over batch/seq)\n",
        "\n",
        "    FLOps: D\n",
        "\n"
      ],
      "metadata": {
        "id": "Lzg6iMI-jQOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "### RMS Norm: Accessed Bytes\n",
        "Let:\n",
        "\n",
        "batch = B, seq = S, hidden = D\n",
        "\n",
        "N = B·S·D\n",
        "\n",
        "1. `Cast:` x.to(float32)\n",
        "\n",
        "    Reads: N · s_in\n",
        "\n",
        "    Writes: N · s32\n",
        "\n",
        "2. `Square:` pow(x32, 2)\n",
        "\n",
        "    Reads: N · s32\n",
        "\n",
        "    Writes: N · s32\n",
        "\n",
        "3. `Mean:` mean(x32^2, dim=-1, keepdim=True) → shape (B,S,1)\n",
        "\n",
        "    Reads: N · s32\n",
        "\n",
        "    Writes: (B·S) · s32\n",
        "\n",
        "4. `Add epsilon:` add(eps) on (B,S,1)\n",
        "\n",
        "    Reads: (B·S) · s32\n",
        "\n",
        "    Writes: (B·S) · s32\n",
        "\n",
        "5. `Reciprocal square root:`  rsqrt on (B,S,1)\n",
        "\n",
        "    Reads: (B·S) · s32\n",
        "\n",
        "    Writes: (B·S) · s32\n",
        "\n",
        "6. `Multiply with x:` mul(x32, inv_r) (broadcast (B,S,1) → (B,S,D))\n",
        "\n",
        "    Reads: N · s32 + (B·S) · s32\n",
        "\n",
        "    Writes: N · s32\n",
        "\n",
        "7. `Retrieve w:` get_attr(weight) (γ, FP32)\n",
        "\n",
        "    Reads: D · 4\n",
        "\n",
        "    Writes: 0\n",
        "\n",
        "8. `Get dtype of original input`\n",
        "\n",
        "    Reads: 0\n",
        "\n",
        "    Writes: 0\n",
        "\n",
        "9. `Cast to original dtype:` y32.to(x.dtype) (back to input dtype)\n",
        "\n",
        "    Reads: N · s32\n",
        "\n",
        "    Writes: N · s_in\n",
        "\n",
        "10. `Multiply elementwise:` mul(y, weight) (broadcast γ over batch/seq)\n",
        "\n",
        "    Reads: N · s_in + D · 4\n",
        "\n",
        "    Writes: N · s_in\n",
        "\n"
      ],
      "metadata": {
        "id": "fEEOjY-EWqJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q K V O Linear Projections\n",
        "Multiply the input tensor with $W^Q$, $W^K$ and $W^V$ learned matrices.\n",
        "\n",
        "These Matrices have the dimension (4096, 4096).\n",
        "\n",
        "After Linear Projection the resulting $Q$, $K$ and $V$ matrices have the same dimensions as the Embedded Input tensor.\n",
        "\n",
        "Source Code:\n",
        "```\n",
        "self.q_proj = nn.Linear(\n",
        "            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n",
        ")\n",
        "self.k_proj = nn.Linear(\n",
        "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
        ")\n",
        "self.v_proj = nn.Linear(\n",
        "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
        ")\n",
        "self.o_proj = nn.Linear(\n",
        "            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n",
        ")\n",
        "```\n",
        "\n",
        "This means we are going to perform a dot product between the hidden state and the matrix weights."
      ],
      "metadata": {
        "id": "GiDubceUi9l7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block = model.model.layers[0]  # Pick any layer, e.g., layer 0\n",
        "\n",
        "q_proj = block.self_attn.q_proj.weight\n",
        "k_proj = block.self_attn.k_proj.weight\n",
        "v_proj = block.self_attn.v_proj.weight\n",
        "o_proj = block.self_attn.o_proj.weight\n",
        "\n",
        "print(\"Query Projection (Q)   :\", q_proj.shape)\n",
        "print(\"Key Projection (K)     :\", k_proj.shape)\n",
        "print(\"Value Projection (V)   :\", v_proj.shape)\n",
        "print(\"Output Projection (O)  :\", o_proj.shape)"
      ],
      "metadata": {
        "id": "s3nzbNwejbhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "### Linear Projection: Static Memory Requirements\n",
        "\n",
        "Weights = 4096 × 4096 = 16,777,216 parameters\n",
        "\n",
        "Bias (if enabled) = 4096 parameters\n",
        "\n",
        "Total parameters = ~16.78M\n",
        "\n",
        "Memory in FP16 (2 bytes per param) = ~32 MB"
      ],
      "metadata": {
        "id": "amIx8OVvgaLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "###Linear Projection: Operations per Token\n",
        "Operations per token for one Linear Layer\n",
        "- For Weights\n",
        "  - Multiplications: 4096 * 4096\n",
        "  - Additions: 4096 * (4096 - 1)\n",
        "- For Bias\n",
        "  - Additions: 4096\n",
        "\n",
        "Total no of Operations per Token 2 * 4096 * 4096\n",
        "\n",
        "= 33,554,432"
      ],
      "metadata": {
        "id": "DQWDtRNxhMTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "###Linear Projection: Accessed Bytes\n",
        "\n",
        "READ:\n",
        "1. Input vector x:\n",
        "\n",
        "  - Shape = (in_features,) = (4096,)\n",
        "\n",
        "  - Size = 4096 × dtype_size\n",
        "\n",
        "  - In FP16 → 4096 × 2B = 8 KB\n",
        "\n",
        "2. Weight matrix W:\n",
        "\n",
        "  - Shape = (out_features, in_features) = (4096, 4096)\n",
        "\n",
        "  - Size = 16,777,216 × 2B = 32 MB\n",
        "\n",
        "3. Bias vector b (if present):\n",
        "\n",
        "  - Shape = (out_features,) = (4096,)\n",
        "\n",
        "  - Size = 4096 × 2B = 8 KB\n",
        "\n",
        "WRITE:\n",
        "\n",
        "1. Output vector y:\n",
        "\n",
        "  - Shape = (out_features,) = (4096,)\n",
        "\n",
        "  - Size = 4096 × 2B = 8 KB"
      ],
      "metadata": {
        "id": "qsAGVcfFjzKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RoPE (Rotary Positional Encoding)\n",
        "\n",
        "  RoPE is applied at $Q$ and $K$ matrices\n",
        "\n",
        "  ![RoPE](https://miro.medium.com/v2/resize:fit:1400/1*jkoR140vi3LncTrVmvdsDQ.png)\n",
        "\n",
        "The cos and sin tensors are precomputed once and reused.\n",
        "- This Rotates the tensor to update the tokens with positional information\n",
        "\n",
        "Source Code:\n",
        "```\n",
        "q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "return q_embed, k_embed\n",
        "```"
      ],
      "metadata": {
        "id": "Uu2k4E0CJKYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi Head Attention (MHA)**\n",
        "\n",
        "  ![G-MQA](https://www.mdpi.com/pharmaceuticals/pharmaceuticals-17-01300/article_deploy/html/images/pharmaceuticals-17-01300-g0A3.png)\n",
        "\n",
        "\n",
        "The 7B version uses MHA, while some higher parameter versions of the model use GQA.\n"
      ],
      "metadata": {
        "id": "3PsxhtLOkQBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Query Tensor is split equally for the Attention heads in the dimension of the encoding.\n",
        "\n",
        "$Q$ dimension = (1, 5, 4096)\n",
        "\n",
        "Split between 32 Attention heads, each head gets an input tensor of dimension (1, 5, 128)\n",
        "\n",
        "$K$  and $V$ are also spilt in a similar way.\n",
        "\n",
        "We apply self attention at each head and finally concatinate the output.\n",
        "\n"
      ],
      "metadata": {
        "id": "owse_MRmrMWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = model.config\n",
        "\n",
        "print(f\"Attention Heads   : {config.num_attention_heads}\")\n",
        "print(f\"KV Heads          : {getattr(config, 'num_key_value_heads', 'N/A')}\")"
      ],
      "metadata": {
        "id": "hS6NOSMDq_gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "###RoPE: Static Memory Requirements\n"
      ],
      "metadata": {
        "id": "Sg7wqmcRmGvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize input text\n",
        "text = \"Hello LLM User\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "input_ids = inputs[\"input_ids\"]\n",
        "\n",
        "# Get rotary embedding module and generate sin, cos\n",
        "seq_len = input_ids.shape[1]\n",
        "rotary_emb = model.model.rotary_emb\n",
        "position_ids = torch.arange(seq_len, dtype=torch.long, device=input_ids.device).unsqueeze(0)\n",
        "cos, sin = rotary_emb(x=input_ids,position_ids=position_ids)\n",
        "\n",
        "print(\"cos shape:\", cos.shape)\n",
        "print(\"sin shape:\", sin.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "kwpFt0V-qBDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically the implementation stores two buffers:\n",
        "\n",
        "cos — shape (max_seq_len, rotary_dim)\n",
        "\n",
        "sin — shape (max_seq_len, rotary_dim)\n",
        "\n",
        "These are stored once and broadcast at runtime.\n",
        "\n",
        "Some implementations keep a shaped/broadcasted view like (1, seq_len, 1, head_dim)like the example above, but it would simply be a subset of the full vector.\n",
        "\n",
        "Total elements = 2 * max_seq_len * rotary_dim\n",
        "\n",
        "Bytes = 2 * max_seq_len * rotary_dim * dtype_size\n",
        "\n",
        "= 2 * 4096 * 128 * 2 = 2MB"
      ],
      "metadata": {
        "id": "yMVjFCkdnfW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "###RoPE: Operations\n",
        "According to the source code we will pwe form 3 Ops per element\n"
      ],
      "metadata": {
        "id": "rU8d8tzkpf9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "###RoPE: Accessed Bytes\n",
        "Per-token per-head:\n",
        "\n",
        "Read Q: 128 * 2 B\n",
        "\n",
        "Read K: 128 * 2 B\n",
        "\n",
        "Read: cos/sin per position = 2 * 128 * 2\n",
        "but shared across heads;\n",
        "per-head share ≈ (2 * 128 * 2) / 32 B\n",
        "\n",
        "Write rotated Q,K: 2 * 128 * 2 B"
      ],
      "metadata": {
        "id": "NtAzAgZ5vla9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention\n",
        "![](https://miro.medium.com/v2/resize:fit:893/1*BKsxsnDbIM7eb_dAtws3Yg.png)\n",
        "\n",
        "Source Code:\n",
        "```\n",
        "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
        "    if attention_mask is not None:\n",
        "        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
        "        attn_weights = attn_weights + causal_mask\n",
        "\n",
        "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n",
        "    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n",
        "    attn_output = torch.matmul(attn_weights, value_states)\n",
        "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "```"
      ],
      "metadata": {
        "id": "0jdCB7LSzeqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed Forward + SwiGLU\n",
        "Until now we have used Self-Attention to enrich a tokens meaning through the words surrounding it and it's position in the sequence.\n",
        "\n",
        "Through a Feed Forward network we can apply a learned transformation at each token independentaly.\n",
        "\n",
        "We can think that Self-attention provids context, while FF provides common knowledge."
      ],
      "metadata": {
        "id": "0EsyNy8JvqJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = model.model.layers[0] # Pick a layer (e.g., layer 0)\n",
        "\n",
        "# Access the MLP (Feed-Forward + SwiGLU)\n",
        "mlp = layer.mlp\n",
        "\n",
        "# Check the components\n",
        "print(\"Gate projection:\", mlp.gate_proj)\n",
        "print(\"Up projection:\", mlp.up_proj)\n",
        "print(\"Down projection:\", mlp.down_proj)"
      ],
      "metadata": {
        "id": "QfsPQZ0FxvUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The steps involved are:\n",
        "1. Linear Projections: Gate Projection and Up Projection (Moving the token vector to a higher embedding space; 11008 vs 4096)\n",
        "2. Followed by SwiGLU activation (Selectively pass through useful information and suppress irrelevant parts)\n",
        "3. At last a down projection to return to the original embedding space"
      ],
      "metadata": {
        "id": "hAGkumusy4qr"
      }
    }
  ]
}