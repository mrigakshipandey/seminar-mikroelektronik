{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuPICYwiGxf9T2NEzhbeXa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrigakshipandey/seminar-mikroelektronik/blob/main/Taks_3_LLaMa_2_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLOLVBrwWRTP",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate huggingface_hub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log in to Hugging Face"
      ],
      "metadata": {
        "id": "4bsFLceKWpoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()  # Paste your Hugging Face token here (with model access)\n"
      ],
      "metadata": {
        "id": "AwpR3eGeWtT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and Load the Model"
      ],
      "metadata": {
        "id": "we_k0-_tXnrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Llama-2-7b-hf\"  # or llama-3 if you have access\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YymqS1TWXfLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print all layers and their details\n",
        "\n",
        "| Parameter                              | Description                                                       | Why It Matters                                                           |\n",
        "| -------------------------------------- | ----------------------------------------------------------------- | ------------------------------------------------------------------------ |\n",
        "| `hidden_size`                          | Dimensionality of the embeddings and hidden layers (e.g. 4096)    | Controls how much information the model can store per token              |\n",
        "| `intermediate_size`                    | Size of the feedforward layer inside each transformer block       | Affects the expressiveness of the MLP sub-layer                          |\n",
        "| `num_hidden_layers`                    | Total number of Transformer blocks (e.g. 32 for LLaMA-2 7B)       | Deeper models usually have better generalization                         |\n",
        "| `num_attention_heads`                  | Number of attention heads per layer                               | Allows model to attend to different subspaces of input                   |\n",
        "| `num_key_value_heads`                  | Number of *shared* KV heads for multi-query attention (MQA)       | Improves inference speed & reduces memory (used in LLaMA 2/3)            |\n",
        "| `vocab_size`                           | Total number of unique tokens the model understands               | Larger vocab supports more languages, rare words                         |\n",
        "| `Max Context`              | Maximum context length (number of tokens) the model can attend to | Limits how long the input/output sequences can be                        |\n",
        "| `Rotary Dim` | Rotary embedding parameter (used for positional encoding)         | Affects how attention handles long-range dependencies                    |\n",
        "| `DropoutPro.`                  | Dropout probability in the hidden layers                          | Regularization to prevent overfitting (not always used during inference) |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RxqUWEUQcTZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = model.config\n",
        "\n",
        "print(\"Model Architecture Summary\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Model Type        : {config.model_type}\")\n",
        "print(f\"Hidden Size       : {config.hidden_size}\")\n",
        "print(f\"Intermediate Size : {config.intermediate_size}\")\n",
        "print(f\"Num of Layers     : {config.num_hidden_layers}\")\n",
        "print(f\"Attention Heads   : {config.num_attention_heads}\")\n",
        "print(f\"KV Heads          : {getattr(config, 'num_key_value_heads', 'N/A')}\")\n",
        "print(f\"Vocab Size        : {config.vocab_size}\")\n",
        "print(f\"Max Context (Seq) : {getattr(config, 'max_position_embeddings', 'N/A')}\")\n",
        "print(f\"Rotary Dim        : {getattr(config, 'rope_theta', 'N/A')}\")\n",
        "print(f\"Dropout Prob.     : {config.hidden_dropout_prob if hasattr(config, 'hidden_dropout_prob') else 'N/A'}\")\n"
      ],
      "metadata": {
        "id": "2h8-O_4AXtj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The General Architecture of LLaMa\n",
        "![llama](https://miro.medium.com/v2/resize:fit:373/1*CQs4ceLpN8tIN8QyezL2Ag.png)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TKl_90lEqC_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for name, module in model.named_modules():\n",
        "    #print(name)\n"
      ],
      "metadata": {
        "id": "pIVqkXMm9sme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input Layer\n",
        "Let us assume an Input tensor of dimensions (1, 5).\n",
        "\n",
        "Where 1 is the Batch size and 5 is the number of token in the Input Sequence"
      ],
      "metadata": {
        "id": "U6UAJCbFB4Uz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding Layer\n",
        "Each token in the Input Sequence is a part of a pre defined vocabulary.\n",
        "\n",
        "We use the position of the token in the vocabulary to perform a lookup operation in the embedding matrix.\n",
        "\n",
        "The embedding matrix: shape `(vocab_size, hidden_size)`.\n",
        "- In our case it's (32000, 4096)\n"
      ],
      "metadata": {
        "id": "9OpKDiHxobNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_layer = model.model.embed_tokens\n",
        "print(\"Embedding matrix shape:\", embed_layer.weight.shape)\n",
        "print(\"Vocab size:\", embed_layer.weight.shape[0])\n",
        "print(\"Hidden size:\", embed_layer.weight.shape[1])"
      ],
      "metadata": {
        "id": "-ioXSjAqb70Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding can be understood to carry the meaning of the token.\n",
        "\n",
        "Given our Input dimensaion (1, 5). After Embedding we'll get a tensor with dimensions (1, 5, 4096).\n",
        "\n",
        "---\n",
        "Additional Notes\n",
        "- In the Original Transformer Architecture, this was followed up by an **Absolute positional Encoding**.\n",
        "\n",
        "- In the LLaMa Model, we use **Rotary Positional Encoding** later with the Query and Key Matrix."
      ],
      "metadata": {
        "id": "PHngHbFvEfvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization\n",
        "Layer Norm uses Recentering and Rescaling, on the contrary RMS norm uses the hypothesis that Rescaling and not Recentering is the major contributing factor for Normalization.\n",
        "\n",
        "- RMS Norm is popular because it requies fewer computations.\n",
        "\n",
        "After Normalizing with the RMS, the output is scaled with a learned vecctor."
      ],
      "metadata": {
        "id": "8ZkY9SWCMGa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rms = model.model.layers[0].input_layernorm  # Pick any layer, e.g., layer 0\n",
        "print(\"Shape:\", rms.weight.shape)"
      ],
      "metadata": {
        "id": "kGWCiJrXsDM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear projections to Q, K, V\n",
        "Multiply the input tensor with $W^Q$, $W^K$ and $W^V$ learned matrices.\n",
        "\n",
        "These Matrices have the dimension (4096, 4096).\n",
        "\n",
        "After Linear Projection the resulting $Q$, $K$ and $V$ matrices have the same dimensions as the Embedded Input tensor."
      ],
      "metadata": {
        "id": "GiDubceUi9l7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block = model.model.layers[0]  # Pick any layer, e.g., layer 0\n",
        "\n",
        "q_proj = block.self_attn.q_proj.weight\n",
        "k_proj = block.self_attn.k_proj.weight\n",
        "v_proj = block.self_attn.v_proj.weight\n",
        "o_proj = block.self_attn.o_proj.weight\n",
        "\n",
        "print(\"Query Projection (Q)   :\", q_proj.shape)\n",
        "print(\"Key Projection (K)     :\", k_proj.shape)\n",
        "print(\"Value Projection (V)   :\", v_proj.shape)\n",
        "print(\"Output Projection (O)  :\", o_proj.shape)"
      ],
      "metadata": {
        "id": "s3nzbNwejbhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi Head Attention (MHA)\n",
        "\n",
        "  ![G-MQA](https://www.mdpi.com/pharmaceuticals/pharmaceuticals-17-01300/article_deploy/html/images/pharmaceuticals-17-01300-g0A3.png)\n",
        "\n",
        "\n",
        "The 7B version uses MHA, while some higher parameter versions of the model use GQA.\n"
      ],
      "metadata": {
        "id": "3PsxhtLOkQBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = model.config\n",
        "\n",
        "print(f\"Attention Heads   : {config.num_attention_heads}\")\n",
        "print(f\"KV Heads          : {getattr(config, 'num_key_value_heads', 'N/A')}\")"
      ],
      "metadata": {
        "id": "hS6NOSMDq_gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Query Tensor is split equally for the Attention heads in the dimension of the encoding.\n",
        "\n",
        "$Q$ dimension = (1, 5, 4096)\n",
        "\n",
        "Split between 32 Attention heads, each head gets an input tensor of dimension (1, 5, 128)\n",
        "\n",
        "$K$  and $V$ are also spilt in a similar way.\n",
        "\n",
        "We apply self attention at each head and finally concatinate the output.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:893/1*BKsxsnDbIM7eb_dAtws3Yg.png)"
      ],
      "metadata": {
        "id": "owse_MRmrMWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RoPE (Rotary Positional Encoding)\n",
        "\n",
        "  RoPE is applied at $Q$ and $K$ matrices\n",
        "\n",
        "  ![RoPE](https://miro.medium.com/v2/resize:fit:1400/1*jkoR140vi3LncTrVmvdsDQ.png)\n",
        "\n",
        "The cos and sin tensors can be precomputed once and reused.\n",
        "- This Rotates the tensor to update the tokens with positional information"
      ],
      "metadata": {
        "id": "Uu2k4E0CJKYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize input text\n",
        "text = \"Hello LLM User\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "input_ids = inputs[\"input_ids\"]\n",
        "\n",
        "# Get rotary embedding module and generate sin, cos\n",
        "seq_len = input_ids.shape[1]\n",
        "rotary_emb = model.model.rotary_emb\n",
        "position_ids = torch.arange(seq_len, dtype=torch.long, device=input_ids.device).unsqueeze(0)\n",
        "cos, sin = rotary_emb(x=input_ids,position_ids=position_ids)\n",
        "\n",
        "print(\"cos shape:\", cos.shape)\n",
        "print(\"sin shape:\", sin.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "kwpFt0V-qBDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-V Cache\n",
        "In Transformer-based autoregressive models (like GPT, LLaMA, etc.), during inference, the model generates one token at a time, step by step, from left to right.\n",
        "\n",
        "At each step:\n",
        "\n",
        "- The model takes all previously generated tokens as input to predict the next token.\n",
        "\n",
        "- Computing attention from scratch over all previous tokens at every step would be very expensive. K-V cache holds the value for previous input token. The output is used as the Query in the next iteration.\n",
        "\n"
      ],
      "metadata": {
        "id": "XXkwP0DakRfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed Forward + SwiGLU\n",
        "Until now we have used Self-Attention to enrich a tokens meaning through the words surrounding it and it's position in the sequence.\n",
        "\n",
        "Through a Feed Forward network we can apply a learned transformation at each token independentaly.\n",
        "\n",
        "We can think that Self-attention provids context, while FF provides common knowledge."
      ],
      "metadata": {
        "id": "0EsyNy8JvqJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = model.model.layers[0] # Pick a layer (e.g., layer 0)\n",
        "\n",
        "# Access the MLP (Feed-Forward + SwiGLU)\n",
        "mlp = layer.mlp\n",
        "\n",
        "# Check the components\n",
        "print(\"Gate projection:\", mlp.gate_proj)\n",
        "print(\"Up projection:\", mlp.up_proj)\n",
        "print(\"Down projection:\", mlp.down_proj)"
      ],
      "metadata": {
        "id": "QfsPQZ0FxvUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The steps involved are:\n",
        "1. Linear Projections: Gate Projection and Up Projection (Moving the token vector to a higher embedding space; 11008 vs 4096)\n",
        "2. Followed by SwiGLU activation (Selectively pass through useful information and suppress irrelevant parts)\n",
        "3. At last a down projection to return to the original embedding space"
      ],
      "metadata": {
        "id": "hAGkumusy4qr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This is performed 32 Times in a row"
      ],
      "metadata": {
        "id": "xWaP_bJC1HKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Final Linear Layer\n",
        "We perform a final Linear Projection. From the output embedding we project the scores for the likelyhood of the succeeding token. These are called logits. To these logits we apply a softmax to convert them into probabilities."
      ],
      "metadata": {
        "id": "ZHlJ1-Kg1aju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"LM head weight shape:\", model.lm_head.weight.shape)"
      ],
      "metadata": {
        "id": "QAhYky5RM5Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d5GLSiL2FuYL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}