{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNLKEdgBUPUHlC++zTeGSz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrigakshipandey/seminar-mikroelektronik/blob/main/Task_1_Related_Literature.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WHAT MATTERS IN TRANSFORMERS? NOT ALL ATTENTION IS NEEDED\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##Assessing Redundancy\n",
        "Redundant modules produce outputs that are similar to their inputs, implying minimal transformation. The similarity between the input $\\mathbf{X}$ and output $\\mathbf{Y}$ of a module is quantified using cosine similarity.\n",
        "\n",
        "Therefor the importance score $\\mathbf{S}$ of the module is computed as:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{S} = 1 - CosineSim(\\mathbf{X},\\mathbf{Y})\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{S} = 1 - \\frac{ \\mathbf{X} ⋅ \\mathbf{Y}}{\\left \\| \\mathbf{X} \\right \\| \\left \\| \\mathbf{Y} \\right \\|}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##Assessing the effects of dropping a module\n",
        "To quantify the trade-off between performance degradation and speedup, we introduce a new metric called Speedup Degradation Ratio or SDR $λ$, defined as:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{S} = \\frac{Δ Avg.}{Δ Speedup}\n",
        "\\end{align}\n",
        "\n",
        "Where $Δ Avg.$ represents the percentage change in average performance across the evaluated tasks and $Δ Speedup$ denotes the corresponding percentage of speedup achieved by each method.\n",
        "\n",
        "##Joint Layer Drop\n",
        "The Joint Layer Drop method is simple Imented by  calculating the importance scores for both attention layers and MLP layers individually. We concatenate the scores and from this combined set of importance scores,we drop the layers with the lowest values.\n",
        "\n",
        "---\n",
        "##Observations\n",
        "- Attention layers are highly redundant, and their removal has minimal impact on model accuracy, making Attention Drop a highly efficient pruning strategy.\n",
        "\n",
        "- Deeper layers (excluding the last ones) often exhibit excessively low importance across Block, MLP, and Attention modules.\n",
        "\n",
        "- Attention layers demonstrate consistently lower importance scores than MLP and Block at all training stages.\n",
        "\n",
        "- Joint Layer Drop  consistently achieves better performance than either Attention Drop or MLP Drop alone.\n",
        "\n",
        "- Given the simplicitynand efficiency,  One-ShotDropping emerges as the superior choice.\n",
        "\n",
        "- Attention Drop is Orthogonal to Quantization.\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "NQ8u6rnb3r_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CirCNN: Accelerating and Compressing Deep Neural Networks Using Block-Circulant Weight Matrices\n",
        "An ideal model compression technique should:\n",
        " -  maintain regular network structure;\n",
        " - reduce the complexity for both inference and training, and, most importantly,\n",
        " - retain a rigorous mathematical fundation on compression ratio and accuracy.\n",
        "\n",
        "CirCNN utilizes the Fast Fourier Transform (FFT)-based fast multiplication, simultaneously reducing the computational complexity (both in inference and\n",
        " training)  and the storage complexity.\n",
        "\n",
        "In a square circulant matrix, each row (or column) vector is the circulant reformat of the other row (column) vectors. A non-squared matrix could be represented by a set of square circulant submatrices (blocks).\n",
        "\n",
        "A fully-connected layer of DNN, can be represented as **y** = ψ(**Wx**+**θ**), where vectors\n",
        "x and y represent the outputs of all neurons in the previous layer and the current layer, respectively; W is the m-by-n weight matrix; and ψ(·) is activation function.\n",
        "\n",
        "When **W** is a block-circulant matrix, the Fast Fourier Transform (FFT)-based fast multiplication method can be utilized, and the computational complexity is reduced. Block circulant matrices can be block-diagonalized using a block FFT. This turns convolution (or multiplication) into element-wise multiplication in the frequency domain, which is much faster.\n",
        "\n",
        "CirCNN directly trains the network assuming block-circulant structure. This leads to two advantages.\n",
        "- CirCNN provides the adjustable but fixed reduction ratioofreductio in model size;\n",
        "- with the same FFT-based fast multiplication, the computational complexity of training is also reduced.\n",
        "\n",
        "To achieve better compression ratio, larger block size should be used, however, it may lead to more accuracy degradation. The smaller block sizes provide better accuracy, but less compression.\n",
        "\n",
        "For CONV Layers, Software tools such as Caffe provide an efficient methodology of transforming tensor-based operations in the CONV layer to matrix\n",
        "based operations.\n",
        "\n",
        "---\n",
        "\n",
        "##Overall Architecture\n",
        "- The **basic computing block** is responsible for the major FFT and IFFT computations.\n",
        "\n",
        "- The **peripheral computing block** is responsible for performing component-wise multiplication, ReLU activation, pooling etc.\n",
        "\n",
        "- The implementations of ReLU activation and pooling are through comparators and have no inherent difference compared with prior work.\n",
        "\n",
        "- The **control subsystem** orchestrates the actual FFT/IFFT calculations on the\n",
        " basic computing block and peripheral computing block. The different setting of FFT/IFFT calculations is configured by the control subsystem.\n",
        "\n",
        "- The **memory subsystem** is composed of ROM, which is utilized to store the coefficients in FFT/IFFT calculations; and RAM, which is used to store weights.\n",
        "\n",
        "- We use 16-bit fixed point numbers for input and weight representations.\n",
        "\n",
        "---\n",
        "\n",
        "##Pipelining and Parallelism\n",
        "- In **inter-level pipelining**, each pipeline stage corresponds to one level in the basic computing block.\n",
        "\n",
        "- In **intra-level pipelining**, additional pipeline stage(s)will  be added with in each butterfly computation unit.\n",
        "\n",
        "- The proper selection of pipelining scheme highly depends on the target operating frequency and memory subsystem organization.\n",
        "\n",
        "- Derive upper bound of $p$ (parallelization degree) based on memory bandwidth-limit & hardware resource limit.\n",
        "\n",
        "- The overall metric $M$, which is a function of performance: $Perf(p,d)$ and  power consumption $Power(p,d)$; where $d$ is the parallelization depth.\n",
        "\n",
        "- We estimate $M$ assuming $d = 1$.\n",
        "\n",
        "- Optimize depth $d$ using the ternary search method, based on the\n",
        " derived $p$ value.\n",
        "\n",
        "---\n",
        "\n",
        "##Platform-Specific Optimizations\n",
        "We focus on weight storage and memory management, in order to simplify the design and achieve higher energy efficiency and performance.\n",
        "\n",
        "###FPGA Platform\n",
        "- Weight storage requirements can be met by the on-chip block memory in state-of-the-art FPGAs.\n",
        "\n",
        "- 16-bit fixed point numbers are used to represent the weights.\n",
        "\n",
        "- Applying Block Circulent Matrix to both FC and CONV layer in AlexNet, the storage requirement can be further reduced to 2MB or even less\n",
        "\n",
        "###ASIC platform\n",
        "- If we target at a clock frequency around 200MHz, then the memory hierarchy is not necessary because a single-level memory system can support such operating frequency.\n",
        "\n",
        "- Memory/cache reconfiguration techniques can be employed when executing different types and sizes of applications for performance enhancement and static power reduction.\n",
        "\n",
        "- If we target at a higher clock frequency, say 800MHz, an effective memory hierarchy with at least two levels (L1 cache and main memory) becomes necessary because a single-level memory cannot accommodate such high operating frequency.\n",
        "\n",
        "- The effectiveness of prefetching is due to the regularity in the proposed block-circulant matrix-based neural networks, showing another advantage over prior compression schemes.\n",
        "\n",
        "- Besides the memory hierarchy structure, the memory bandwidth is determined by the parallelization degree $p$ in the basic computing block.\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "L1oroexjSAiA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Xa0q0foIDb8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}